{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# delete this block\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "%autosave 0"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction to spatial and spatiotemporal data in Python\n",
        "\n",
        "This session will be used to go over the basics of accessing and manipulating spatial data with Python, using standard tools in the current Python geospatial ecosystem:\n",
        " - `rasterio` - for reading/writing raster data (among other things)\n",
        " - `numpy` - for manipulating N-dimensional arrays\n",
        " - `geopandas` - for reading/writing vector data and manipulating them in the form of a `pandas.DataFrame` with added functionality\n",
        "\n",
        "Additionally, we will introduce some ease-of-use and performance oriented functionality implemented within `eumap`.\n",
        "\n",
        "## Raster manipulation\n",
        "\n",
        "First, we will open and inspect a raster with `rasterio`, a library that uses `GDAL` under the hood but provides an interface closer to idiomatic Python.\n",
        "\n",
        "Let's gather some raster file paths."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "from pathlib import Path\n",
        "from eumap.misc import find_files\n",
        "\n",
        "data_home = Path.home()/'ODSE_workdir/data'\n",
        "tile_id = 5606\n",
        "\n",
        "tile_dir = data_home/f'tile_{tile_id}'\n",
        "\n",
        "raster_paths = find_files(tile_dir, 'lcv/*.tif')\n",
        "\n",
        "print('N files:', len(raster_paths))\n",
        "print()\n",
        "for rpath  in raster_paths[:10]:\n",
        "    print(rpath)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will open the second file on the list. This provides us with a `rasterio.DatasetReader` object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "import rasterio as rio\n",
        "\n",
        "raster = rio.open(raster_paths[1])\n",
        "print(raster)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Aside from providing an interface for reading data, the `DatasetReader` contains useful metadata."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "print('driver:', raster.driver)\n",
        "print('N bands:', raster.count)\n",
        "print('shape:', raster.shape)\n",
        "print('CRS:', raster.crs)\n",
        "print('transform:', raster.transform)\n",
        "print('nodata:', raster.nodata)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "...all of which (and more) is available inside the `profile` property."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "print(raster.profile)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calling the `read()` method with the band index as the argument (starting from 1) provides us with a `numpy` array containing the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "data = raster.read(1)\n",
        "print(data, type(data))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As the data is read into a `numpy.ndarray`, we can perform the same operations as with any other `numpy` array, like computing statistics with the array's methods.\n",
        "\n",
        "We can visualize the data with `eumap.plotter`, which provides a wrapper around `matplotlib` that ensures image aspect ratio is preserved and provides some added functionality, like transparency on `nodata`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "from eumap import plotter\n",
        "import numpy as np\n",
        "\n",
        "# in case of plots not appearing properly in Jupyter\n",
        "%matplotlib inline\n",
        "\n",
        "data_min = data.min()\n",
        "data_max = data.max()\n",
        "data_median = np.median(data)\n",
        "\n",
        "nodata = raster.nodata\n",
        "\n",
        "plotter.plot_rasters(\n",
        "    data,\n",
        "    titles=f'min: {data_min}, max: {data_max}, median: {data_median}',\n",
        "    figsize=5,\n",
        "    nodata=nodata,\n",
        "    vmin=int(data_min),\n",
        "    vmax=int(data_max),\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Aside from providing an easy way to deal with arrays and access to linear algebra, `numpy` supplies highly performant vectorized operations using BLAS libraries like MKL and OpenBLAS under the hood.\n",
        "\n",
        "Let's find out where our data falls outside the interval from the 20th to the 80th percentile. Comparing an array with a number (or another array with compatible dimensions) yields an array of boolean elements. We can perform operations on these boolean arrays with Python's bitwise boolean operators."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "low, high = np.percentile(data, [20, 80])\n",
        "print('P5:', low)\n",
        "print('P95:', high)\n",
        "\n",
        "hi_index = data > high\n",
        "lo_index = data < low\n",
        "nodata_index = hi_index | lo_index\n",
        "print('index:', nodata_index)\n",
        "\n",
        "pct_outside = 100 * nodata_index.sum() / nodata_index.size\n",
        "print('% of data outside of bounds:', pct_outside.round(2))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using the index we can now alter the out-of-bounds pixels to the nodata value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "new_data = data.copy()\n",
        "new_data[nodata_index] = nodata\n",
        "\n",
        "plotter.plot_rasters(\n",
        "    new_data,\n",
        "    titles=f'nodata between 20th and 80th percentile',\n",
        "    figsize=5,\n",
        "    nodata=nodata,\n",
        "    vmin=int(data_min),\n",
        "    vmax=int(data_max),\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "...or clip the data to the interval."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "new_data[lo_index] = low\n",
        "new_data[hi_index] = high\n",
        "\n",
        "plotter.plot_rasters(\n",
        "    new_data,\n",
        "    titles=f'data clipped between 20th and 80th percentile',\n",
        "    figsize=5,\n",
        "    nodata=nodata,\n",
        "    vmin=int(data_min),\n",
        "    vmax=int(data_max),\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice that the areas previously containing no data are now filled with valid values. That's because we didn't account for that nodata mask. We can produce an index by either comparing the data array to the nodata value, or better yet, using the `read_masks()` method of the `DatasetReader`. While nodata masks can sometimes be accounted for later on, it is beneficial to conserve resources by avoiding computation on nodata altogether."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "data_mask = raster.read_masks(1).astype(bool)\n",
        "data_only = data[data_mask].copy()\n",
        "\n",
        "hi_index = data_only > high\n",
        "lo_index = data_only < low\n",
        "\n",
        "data_only[lo_index] = low\n",
        "data_only[hi_index] = high\n",
        "\n",
        "new_data[:] = nodata\n",
        "new_data[data_mask] = data_only\n",
        "\n",
        "plotter.plot_rasters(\n",
        "    new_data,\n",
        "    titles=f'data clipped between 20th and 80th percentile',\n",
        "    figsize=5,\n",
        "    nodata=nodata,\n",
        "    vmin=int(data_min),\n",
        "    vmax=int(data_max),\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's open a folder and write our new data to a file. To open a file in write mode, we need to provide additional arguments, such as raster width and height in pixels, driver, etc. A transformation matrix and CRS are also required if we want our raster to be properly geospatially referenced. Luckily, all of this is contained in the `profile` `dict` of our `DatasetReader`. Since we have not changed any properties of the raster other than the data itself, we can pass the entire `profile` to the `DatasetWriter` as `**kwargs`.\n",
        "\n",
        "Note that `rasterio` readers and writers can also be used as context managers (as one would use `open()` from the Python standard library)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "\n",
        "out_dir = data_home/'session_2_outputs'\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "out_path = out_dir/'raster.tif'\n",
        "\n",
        "with rio.open(out_path, 'w', **raster.profile) as dst:\n",
        "    print(dst)\n",
        "    dst.write(new_data, 1)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`plotter.plot_rasters()` can also be called with dataset filepaths instead of data arrays. When used this way we do not have to provide the `nodata` argument, as it will be read from the file automatically."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "plotter.plot_rasters(\n",
        "    out_path,\n",
        "    figsize=5,\n",
        "    vmin=int(data_min),\n",
        "    vmax=int(data_max),\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Vector manipulation\n",
        "\n",
        "For vector reading/writing and manipulation, we will use `geopandas`, a library that extends data frames with capabilities for processing geometries in a vectorized manner. `geopandas` internally uses `fiona` for I/O and `shapely` (and more recently `pygeos`) for handling geometry primitives.\n",
        "\n",
        "Let's load the sample land cover points provided for the workshop and inspect the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "import geopandas as gp\n",
        "\n",
        "points = gp.read_file(data_home/'land_cover_samples.gpkg')\n",
        "\n",
        "print('CRS:', points.crs)\n",
        "print('N points:', points.index.size)\n",
        "\n",
        "points"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can compare the extent of the points with that of our sample raster. We can use `shapely` to construct polygons from the bounding box coordinates of both datasets. `shapely` interacts nicely with Jupyter and visualizes geometries on output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "from shapely.geometry import box, MultiPolygon\n",
        "\n",
        "raster_extent = box(*raster.bounds)\n",
        "print('raster extent:', raster_extent)\n",
        "\n",
        "points_bounds = points.cascaded_union.bounds\n",
        "points_extent = box(*points_bounds)\n",
        "print('points extent:', points_extent)\n",
        "\n",
        "MultiPolygon([raster_extent, points_extent])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see the extent of the points is much larger than that of the raster, so we will utilize `geopandas` to perform a vectorized intersection check over the points. This produces a boolean array which we can use to index only the points inside the raster bounds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "point_subset_index = points.intersects(raster_extent)\n",
        "\n",
        "point_subset = points[point_subset_index]\n",
        "\n",
        "print('N points:', point_subset.index.size)\n",
        "point_subset.cascaded_union"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now write our reduced point dataset to a file. Like `rasterio` (or `GDAL`), `geopandas` has support for various drivers. Here we will output the points to GeoJSON."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "point_subset.to_file(\n",
        "    out_dir/'clipped_points.geojson',\n",
        "    driver='GeoJSON'\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Computing a raster time series\n",
        "\n",
        "Let's compute a spring NDVI timeseries for our tile from LANDSAT composites for the spring season for years 2000 to 2020. We can use `eumap.raster.read_rasters()` for a multithreaded read of multiple datasets into a single array. `read_rasters()` behaves in a time series friendly manner, stacking all layers into a multiband image. It also takes care of nodata masks, filling them with NaN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "from eumap.raster import read_rasters\n",
        "\n",
        "red_files = find_files(tile_dir, 'lcv/lcv_red_landsat.glad.ard_p50*03.21*.tif')\n",
        "nir_files = find_files(tile_dir, 'lcv/lcv_nir_landsat.glad.ard_p50*03.21*.tif')\n",
        "\n",
        "red, __ = read_rasters(raster_files=red_files)\n",
        "nir, __ = read_rasters(raster_files=nir_files)\n",
        "\n",
        "print('array shapes:', red.shape, nir.shape)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now use the stacked data to compute the entire NDVI series at once. We will then plot the series at a single pixel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn')\n",
        "\n",
        "ndvi = (nir - red) / (nir + red)\n",
        "\n",
        "years = [*range(2000, 2021)]\n",
        "\n",
        "def plot_series(data, index):\n",
        "    xi, yi = index\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(years, data[yi,xi,:])\n",
        "    ax.set_title(f'NDVI series at pixel {index}')\n",
        "    ax.set_xticklabels(years, rotation=90)\n",
        "\n",
        "plot_series(ndvi, (500, 500))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also use `plotter` to plot the series over the entire tile, but first we have to unstack the image into separate arrays for each year."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "ndvi = np.moveaxis(ndvi, -1, 0)\n",
        "ndvi.shape"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will now plot the series over the last five years."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "plotter.plot_rasters(\n",
        "    *ndvi.astype(np.float32)[-5:],\n",
        "    figsize=10,\n",
        "    titles=years[-5:],\n",
        "    cmaps='Greens',\n",
        "    vmin=-1,\n",
        "    vmax=1,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's compute for each year the NDVI difference to the previous one and plot the results for the last five years."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "diff = ndvi[1:] - ndvi[:-1]\n",
        "\n",
        "plotter.plot_rasters(\n",
        "    *diff.astype(np.float32)[-5:],\n",
        "    figsize=10,\n",
        "    titles=years[-5:],\n",
        "    cmaps='RdYlGn',\n",
        "    vmin=-.5,\n",
        "    vmax=.5,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use `save_rasters()` to batch write results for all the years in parallel, analogue to `read_rasters()`. `save_rasters()` takes series as multiband images, so we have to stack our `diff` array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "from eumap.raster import save_rasters\n",
        "\n",
        "out_files = [out_dir/f'ndvi_diff/ndvi_diff_{year}.tif' for year in years[1:]]\n",
        "\n",
        "save_rasters(\n",
        "    raster.name,\n",
        "    out_files,\n",
        "    np.stack(diff, -1).astype(np.float32),\n",
        "    dtype='float32',\n",
        "    nodata=-99.,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's overlay the results with our point subset. `eumap` provides a parallelized way of overlaying points with rasters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "from eumap.mapper import SpaceOverlay\n",
        "from datetime import datetime\n",
        "overlay_points = point_subset\n",
        "\n",
        "overlay = SpaceOverlay(\n",
        "    point_subset[['geometry']],\n",
        "    out_files,\n",
        ").run()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each point will now have attached the time series of NDVI differences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "overlay"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's plot the series at one of the points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "def plot_series(point_id):\n",
        "    diff_cols = sorted(overlay.columns[2:])\n",
        "    fig, ax = plt.subplots()\n",
        "    series = overlay.loc[point_id][diff_cols].values\n",
        "    ax.plot(years[1:], series)\n",
        "    ax.set_title(f'NDVI compared to previous year at point {point_id}')\n",
        "    ax.set_xticklabels(years[1:], rotation=90)\n",
        "\n",
        "plot_series(1990)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Raster block processing\n",
        "\n",
        "`rasterio` allows for rasters to be read from within a defined window."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "from rasterio.windows import Window\n",
        "\n",
        "window = Window(\n",
        "    col_off=0,\n",
        "    row_off=0,\n",
        "    width=5,\n",
        "    height=5,\n",
        ")\n",
        "\n",
        "window_data = raster.read(1, window=window)\n",
        "\n",
        "window_data"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cloud optimized GeoTIFFs are internally organized into blocks with local compression. Let's check the block number of our raster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "block_windows = [*raster.block_windows()]\n",
        "\n",
        "print('N blocks:', len(block_windows))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When reading a window from a raster, all blocks intersecting the window have to be read and decompressed. So if we read a single raster in parallel but only at windows corresponding to block boundaries, we minimize read time. `eumap.parallel.blocks` leverages this to enable efficient processing of large datasets, which allows for both faster processing on large hardware infrastructure and for long-running processing with limited resources.\n",
        "\n",
        "We will compute NDVI for a single season within the boundary of our raster, but this time from pan-european LANDSAT mosaics hosted in S3 buckets.\n",
        "\n",
        "To do this we first have to convert the geometry into the GeoJSON schema."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "from shapely.geometry import mapping\n",
        "\n",
        "red_url = 'http://s3.eu-central-1.wasabisys.com/eumap/landsat/landsat_ard_20180625_20180912_red_p50.tif'\n",
        "nir_url = 'http://s3.eu-central-1.wasabisys.com/eumap/landsat/landsat_ard_20180625_20180912_nir_p50.tif'\n",
        "\n",
        "with rio.open(red_url) as src:\n",
        "    print('raster size:', src.shape)\n",
        "\n",
        "geometry = mapping(raster_extent)\n",
        "\n",
        "print(geometry)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will now initialize the reader and writer and define NDVI as a function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "from eumap.parallel.blocks import RasterBlockReader, RasterBlockWriter\n",
        "\n",
        "def calc_ndvi(red, nir):\n",
        "    return (nir - red) / (nir + red)\n",
        "\n",
        "reader = RasterBlockReader(reference_file=red_url)\n",
        "writer = RasterBlockWriter(reader=reader)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now start the block-wise processing and write the result to a file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "out_file = out_dir/'ndvi_blocks.tif'\n",
        "\n",
        "writer.write(\n",
        "    src_path=[red_url, nir_url],\n",
        "    dst_path=out_file,\n",
        "    geometry=geometry,\n",
        "    block_func=calc_ndvi,\n",
        "    nodata=-9999.,\n",
        "    dtype='float32',\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's check the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "plotter.plot_rasters(\n",
        "    out_file,\n",
        "    cmaps='Greens',\n",
        "    figsize=5,\n",
        "    vmin=-1,\n",
        "    vmax=1,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ODS data catalogue\n",
        "\n",
        "`eumap.datasets.Catalogue` provides some abstraction over the ODS data catalogue. It provides a search utility to access dataset URLs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "from eumap.datasets import Catalogue\n",
        "\n",
        "cat = Catalogue(use_csw=False)\n",
        "results = cat.search('ndvi')\n",
        "\n",
        "results"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can ommit unwanted results with the `exclude` keyword argument"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "results = cat.search('ndvi', exclude=['trend'])\n",
        "\n",
        "results"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "...and also search by year."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "results = cat.search(\n",
        "    'ndvi',\n",
        "    exclude=['trend'],\n",
        "    years=[2019]\n",
        ")\n",
        "\n",
        "results"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since the results behave more or less like regular Python strings, we can sort them into a time series and read with `read_rasters`. We will read the window corresponding to our test raster"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from rasterio.windows import from_bounds\n",
        "\n",
        "results = sorted(results)\n",
        "\n",
        "ref = rio.open(results[0])\n",
        "\n",
        "window = from_bounds(\n",
        "    *raster_extent.bounds,\n",
        "    transform=ref.transform,\n",
        ")\n",
        "\n",
        "q_ndvi, __ = read_rasters(\n",
        "    raster_files=[*map(str, results)],\n",
        "    spatial_win=window,\n",
        "    dtype=ref.profile['dtype'],\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "...and plot the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "plotter.plot_rasters(\n",
        "    *np.moveaxis(q_ndvi, -1, 0),\n",
        "    figsize=5,\n",
        "    cmaps='Greens',\n",
        "    nodata=ref.nodata,\n",
        "    vmin=int(q_ndvi.min()),\n",
        "    vmax=int(q_ndvi.max()),\n",
        ")\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}